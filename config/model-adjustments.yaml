# model-adjustments.yaml
adjustments:
  # Top Tier Coding Models (Professional Grade)
  "gpt-4-turbo-preview": 3000          # OpenAI's best coding model (128k context)
  "gpt-4-0613": 2800                   # Original GPT-4 code interpreter
  "claude-3.5-sonnet": 3200            # Anthropic's new coding leader
  "codestral-22b-v0.1": 2900           # Mistral's coding specialist
  "codellama-70b-instruct": 2700        # Meta's top code model
  "deepseek-coder-33b-instruct": 2600   # Best at fill-in-middle coding
  "codeqwen1.5-72b-chat": 2500          # Qwen's top coding model
  "starcoder2-15b": 2300               # BigCode's newest iteration

  # Strong Mid-Tier Coding Models
  "mixtral-8x22b-instruct": 2400       # Large context (64k) coding
  "wizardcoder-33b-v1.1": 2200
  "magicoder-s-ds-6.7b": 2100          # Synthetic code-trained model
  "phind-code-llama-34b-v2": 2000

  # Chat-Optimized Coding Models
  "gpt-3.5-turbo-0125": 1500           # OpenAI's fastest coder
  "claude-2.1": 1300                   # Previous gen Claude
  "codebooga-34b": 1800

  # Specialized Coding Models
  "starcoder-3b": 800                  # Lightweight but capable
  "replit-code-v1.5-3b": 700
  "codet5p-16b": 600

  # Deprioritize Non-Coding/Base Models
  "embed-": -5000                      # All embedding models
  "dall-e": -5000                      # Image models
  "whisper": -5000                     # Audio models
  "tts-": -5000                        # Text-to-speech
  "moderation": -5000                  # Content filters
  "text-davinci-003": -3000            # Old completion-style
  "llama-2-7b": -2000                  # Base non-code tuned

  # Version-Specific Adjustments
  "gpt-4-32k": 2800                    # Legacy 32k context
  "claude-3-opus": 2800                # Powerful but not coding-specialized
  "mixtral-8x7b-instruct": 1900        # Previous Mixtral

  # Vendor-Specific Coding Models
  "gemma-code-2b": 1200                # Google's code model
  "xwin-coder-34b": 1600
  "solar-10.7b-instruct": 1400

  # Deprioritize Non-Chat Formats
  "-base": -3000                       # Base non-instruct models
  "-completion": -3000                 # Completion-style endpoints
  "-pretrained": -3000
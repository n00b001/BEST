# model-adjustments.yaml
adjustments:
  # Top Tier Coding Models (Professional Grade)
  "gpt4turbopreview": 100          # OpenAI's best coding model (128k context)
  "deepseekr1": 100
  "DeepSeekR1Zero": 100
  "claude3.5sonnet": 100            # Anthropic's new coding leader
  "codestral2501": 90
  "codestral2412": 85
  "codestral": 80
  "gpt40613": 95                   # Original GPT-4 code interpreter
  "codestral22bv0.1": 97           # Mistral's coding specialist
  "codellama70binstruct": 90        # Meta's top code model
  "deepseekcoder33binstruct": 88   # Best at fill-in-middle coding
  "codeqwen1.572bchat": 85          # Qwen's top coding model
  "starcoder215b": 80               # BigCode's newest iteration
  "geminipro": 50
  "gemini": 40
  "mistralsmall": 30
  "mistral": 40
  "llama": 30
  "qwen2.5": 60
  "qwen-2.5": 60
  "qwen-2.5-coder": 60
  "qwen-2.5-coder-32b": 60
  "qwen-2.5-coder-32b-instruct": 60
  "tiny": -10
  "small": -5
  "medium": 0
  "large": 30
  "coder": 30
  "qwq": 60
  "deepseekv3": 80
  "olympiccoder": 10

  # Strong Mid-Tier Coding Models
  "mixtral8x22binstruct": 84       # Large context (64k) coding
  "wizardcoder33bv1.1": 78
  "magicodersds6.7b": 77          # Synthetic code-trained model
  "phindcodellama34bv2": 75   # Chat-Optimized Coding Models
  "gpt3.5turbo0125": 50           # OpenAI's fastest coder
  "claude2.1": 40                   # Previous gen Claude
  "codebooga34b": 60

  # Specialized Coding Models
  "starcoder3b": 25                  # Lightweight but capable
  "replitcodev1.53b": 23
  "codet5p16b": 20

  # Deprioritize Non-Coding/Base Models
  "embed": -100                      # All embedding models
  "dalle": -100                      # Image models
  "whisper": -100                     # Audio models
  "tts": -100                        # Text-to-speech
  "moderation": -100                  # Content filters
  "textdavinci003": -30             # Old completion-style
  "llama27b": -20                  # Base non-code tuned
  "auto": -3000                       # openrouter auto
  "flux": -3000                       # openrouter auto
  "stablediffusion": -100

  # Version-Specific Adjustments
  "gpt432k": 95                    # Legacy 32k context
  "claude3opus": 95                # Powerful but not coding-specialized
  "mixtral8x7binstruct": 63         # Previous Mixtral

  # Vendor-Specific Coding Models
  "gemmacode2b": 40                # Google's code model
  "xwincoder34b": 52
  "solar10.7binstruct": 47

  # Deprioritize Non-Chat Formats
  "base": -30                       # Base non-instruct models
  "completion": -30                 # Completion-style endpoints
  "pretrained": -30
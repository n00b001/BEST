import os

PORT = 12345
API_TIMEOUT_SECS = 300
LOG_LEVEL = "INFO"
DOT_ENV_FILENAME = ".env"
DEFAULT_CONFIG_FOLDER = "config"
PROVIDERS_CONFIG_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "providers.yaml")
META_PROVIDERS_CONFIG_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "meta-providers.yaml")
MODEL_ADJUSTMENTS_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "model-adjustments.yaml")
DEFAULT_COOLDOWN_SECONDS = 10
BAD_REQUEST_COOLDOWN_SECONDS = 300
EXTERNAL_HEALTHCHECK_URL = "https://free-llm-api.onrender.com/ok"
NON_PROJECT_HEALTHCHECK_URL = "http://example.com"
MAX_REQUEST_CHAR_COUNT_FOR_LOG = 100
GENERATED_PROVIDER_CONFIG_STALE_TIME_SECS = 60 * 60 * 24
DEBUG_MODE = False
MAX_RETRIES = 30
RATE_LIMIT_BACKOFF_SECS = 1

# model params are typically in the range: 0.5b -> 405b
MODEL_PARAM_SCORE_SCALAR = 100 / 405
# context is typicall in the range: 1024 -> 2_000_000
MODEL_CTX_SCORE_SCALAR = 100 / 2_000_000
# score is typically in the range: 0 -> 1
MODEL_LEADERBOARD_SCORE_SCALAR = 100 / 1
# in the range: (-99999 -> 100)
MODEL_ADJUSTMENT_SCALAR = (100 / 100) * 2

import os

PORT = 12345
API_TIMEOUT_SECS = 300
LOG_LEVEL = "INFO"
DOT_ENV_FILENAME = ".env"
DEFAULT_CONFIG_FOLDER = "config"
PROVIDERS_CONFIG_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "providers.yaml")
META_PROVIDERS_CONFIG_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "meta-providers.yaml")
MODEL_ADJUSTMENTS_FILENAME = os.path.join(DEFAULT_CONFIG_FOLDER, "model-adjustments.yaml")
DEFAULT_COOLDOWN_SECONDS = 30
BAD_REQUEST_COOLDOWN_SECONDS = 300
EXTERNAL_HEALTHCHECK_URL = "https://free-llm-api.onrender.com/ok"
NON_PROJECT_HEALTHCHECK_URL = "http://example.com"
MAX_REQUEST_CHAR_COUNT_FOR_LOG = 100
GENERATED_PROVIDER_CONFIG_STALE_TIME_SECS = 60 * 60 * 24

# model params are typically in the range: 0.5b -> 405b
MODEL_PARAM_SCORE_SCALAR = 0.1
# context is typicall in the range: 1024 -> 1000000
MODEL_CTX_SCORE_SCALAR = 0.00001
MODEL_LEADERBOARD_SCORE_SCALAR = 0.1
MODEL_ADJUSTMENT_SCALAR = 1
